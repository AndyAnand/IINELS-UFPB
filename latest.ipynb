{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sci2\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as mtp\n",
    "import random as rng\n",
    "import functools\n",
    "from sklearn import svm\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.feature_selection import RFE, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import os\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transforma tempo em índice de array \n",
    "#    seconds => tempo em segundos\n",
    "#    fs => frequencia de amostragem\n",
    "def indexFromSeconds(seconds, fs):\n",
    "    return np.multiply(seconds,fs)\n",
    "\n",
    "# calcula o intervalo médio entre spikes\n",
    "#    spikes => array crescente com os tempos (em qualquer unidade) de ocorrência de spikes em um determinado neurônio\n",
    "#    retorna 0 se o número de spikes for <= 1\n",
    "#    retorna o intervalo médio, caso contrário\n",
    "def spikesLatency(spikes):\n",
    "    if len(spikes) <= 1:\n",
    "        return 0.0\n",
    "    s = 0\n",
    "    latencies = []\n",
    "    for i in range(0, len(spikes) - 1):\n",
    "        latencies.append(spikes[i + 1] - spikes[i])\n",
    "        s += spikes[i + 1] - spikes[i]\n",
    "    return s/len(spikes)\n",
    "\n",
    "# extrai a somatório do espectro de potência nas faixas: [4-8] Hz, [8-12] Hz, [12-40] Hz, [40-100] Hz\n",
    "# flp1 => np array com 4 colunas cada uma contendo um sinal lfp         Obs.: Convenção linha x coluna: (linha, coluna)\n",
    "# fs => frequência de amostragem\n",
    "# OBS.: essa função pode ser generalizada para um número arbitrário de lfps e para bandas de frequência arbitrárias\n",
    "def lfpBandsExtractor(flp1, fs):\n",
    "    #bands = [[0,4], [4,8], [8,12], [12,40], [40,100]]\n",
    "    bands = [[4,8], [8,12], [12,40], [40,100]]        #excluindo banda delta\n",
    "    bandsV = []\n",
    "    for j in range(0, 4):\n",
    "        f = np.fft.fft(flp1[:, j])\n",
    "        f = np.abs(f)\n",
    "        f = f * f\n",
    "\n",
    "        time_step = 1/fs\n",
    "        freqs = np.fft.fftfreq(len(flp1), time_step)\n",
    "        idx = np.argsort(freqs)\n",
    "        x = freqs[idx]\n",
    "        y = f[idx]\n",
    "\n",
    "        i = 0\n",
    "        #mtp.plot(x, y)\n",
    "        #mtp.xlabel('Frequência')\n",
    "        #mtp.ylabel('Potência')\n",
    "        #np.savetxt(fname='LFP_power_example', delimiter='\\n', fmt='%.12f', newline='\\n', X=y)\n",
    "        #mtp.clf()\n",
    "        #mtp.plot(flp1[:,0])\n",
    "        #mtp.xlabel('Tempo')\n",
    "        #mtp.ylabel('Potencial')\n",
    "        #np.savetxt(fname='LFP_example', delimiter='\\n', fmt='%.12f', newline='\\n', X=flp1[:,0])\n",
    "        for b in bands:\n",
    "            bandsum = 0\n",
    "            while(x[i] < b[0]):\n",
    "                i += 1\n",
    "            while(x[i] < b[1]):\n",
    "                bandsum += y[i]\n",
    "                i += 1\n",
    "            bandsV.append(bandsum)\n",
    "    \n",
    "    return bandsV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Função não mais utilizada. Primeira implementação do classificador onde os dados pré eram separados dos pós\n",
    "\n",
    "def classifyOverlapping(mat_full, mat, trigtimes, fs, interesting_time, window_size, overlap):\n",
    "    \n",
    "    if (overlap > interesting_time - window_size):\n",
    "        raise Error('Erro')\n",
    "    if (overlap > 0):\n",
    "        numberOfLoops = int((interesting_time-window_size)/overlap)   #a divisão tem que dar inteira, atenção\n",
    "    else:\n",
    "        numberOfLoops = 0\n",
    "    print(numberOfLoops)\n",
    "    \n",
    "    data_header = []\n",
    "    for l in spikes_labels:\n",
    "        data_header.append(l + '_num_spikes')\n",
    "        data_header.append(l + '_lat_spikes')\n",
    "    data_header.append('delta_power1')\n",
    "    data_header.append('theta_power1')\n",
    "    data_header.append('alpha_power1')\n",
    "    data_header.append('beta_power1')\n",
    "    data_header.append('gamma_power1')\n",
    "    data_header.append('delta_power2')\n",
    "    data_header.append('theta_power2')\n",
    "    data_header.append('alpha_power2')\n",
    "    data_header.append('beta_power2')\n",
    "    data_header.append('gamma_power2')\n",
    "    data_header.append('delta_power3')\n",
    "    data_header.append('theta_power3')\n",
    "    data_header.append('alpha_power3')\n",
    "    data_header.append('beta_power3')\n",
    "    data_header.append('gamma_power3')\n",
    "    data_header.append('delta_power4')\n",
    "    data_header.append('theta_power4')\n",
    "    data_header.append('alpha_power4')\n",
    "    data_header.append('beta_power4')\n",
    "    data_header.append('gamma_power4')\n",
    "    data_header.append('decision')\n",
    "    \n",
    "    total_scores = []\n",
    "    better_total_scores = []\n",
    "    for loop in range(0, numberOfLoops + 1):   # +1 accounts for the first window\n",
    "        eventIndexes = trigtimes[:,0]\n",
    "        results = ['D', 'E']\n",
    "        dataframe = []\n",
    "\n",
    "        searchIndexes = [0] * len(spikes_labels)\n",
    "\n",
    "        for j in range(0, len(eventIndexes)):\n",
    "            print(\"\\r\",j,end='')\n",
    "            event_features = []\n",
    "            for i in range(0, len(spikes_labels)):\n",
    "                spikeTimes = np.array(mat_full[spikes_labels[i]]) #in seconds\n",
    "                #fft lfp attributes\n",
    "                spikesCount = 0\n",
    "                posSpikesCount = 0\n",
    "                preSpikesList = []\n",
    "                posSpikesList = []\n",
    "                \n",
    "                #pre\n",
    "                #preSpikesList = spikeTimes[(spikeTimes >= eventIndexes[j] - interesting_time + (loop * overlap)) &\n",
    "                #                          (spikeTimes < eventIndexes[j] - interesting_time + window_size + (loop * overlap))]\n",
    "                #pos\n",
    "                posSpikesList = spikeTimes[(spikeTimes >= eventIndexes[j] + (loop*overlap)) &\n",
    "                                           (spikeTimes < eventIndexes[j] + window_size + (loop * overlap))]\n",
    "                \n",
    "                #event_features.append(len(preSpikesList))\n",
    "                event_features.append(len(posSpikesList))\n",
    "                #event_features.append(spikesLatency(preSpikesList))\n",
    "                \n",
    "                event_features.append(spikesLatency(posSpikesList))    \n",
    "            #pre    \n",
    "            #event_features = event_features + lfpBandsExtractor(\n",
    "            #    mat[eventIndexes[j]*fs - interesting_time*fs + loop*overlap*fs:\n",
    "            #        eventIndexes[j]*fs - interesting_time*fs +  window_size*fs + loop*overlap*fs], fs)\n",
    "            #pos\n",
    "            event_features = event_features + lfpBandsExtractor(\n",
    "                mat[eventIndexes[j]*fs + loop*overlap*fs:\n",
    "                    eventIndexes[j]*fs + window_size*fs + loop*overlap*fs], fs)\n",
    "                \n",
    "            event_features.append(labels[j,0])\n",
    "            dataframe.append(event_features)\n",
    "\n",
    "            \n",
    "        print(np.shape(dataframe))\n",
    "        \n",
    "        #normalizing\n",
    "        \n",
    "        features_set = np.asarray(dataframe)        \n",
    "        \n",
    "        print(len(features_set))       \n",
    "        \n",
    "        print(\"    Rodando database normal...\")\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(features_set), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(features_set[train,0:np.shape(features_set)[1] - 1])\n",
    "            clf.fit(scaler.transform(features_set[train,0:np.shape(features_set)[1] - 1]), features_set[train, np.shape(features_set)[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(features_set[test, 0:np.shape(features_set)[1] - 1]))\n",
    "            scored = predictions == features_set[test, np.shape(features_set)[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "            #print(predictions)\n",
    "        #scores = cross_validation.cross_val_score(\n",
    "        #clf, X_scaled, features_set[:,164], cv=5)\n",
    "        print(\"Taxa de acerto média \", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        total_scores.append(sum(all_scores)/len(all_scores))\n",
    "        \n",
    "        print(\"    Rodando database com os melhores atributos...\")\n",
    "        \n",
    "        dataset_std = preprocessing.StandardScaler().fit_transform(features_set[:,0:np.shape(features_set)[1] - 1])\n",
    "        model = LogisticRegression()\n",
    "        rfe = RFE(model, 1)\n",
    "        fit = rfe.fit(dataset_std, features_set[:, np.shape(features_set)[1] - 1])\n",
    "        data_header = np.array(data_header)\n",
    "        rankeds = np.argsort(fit.ranking_)\n",
    "        \n",
    "        np.savetxt(spikes_files[f_index].split('.')[0] + '_pos_ranking_features_' + str(loop) + '.csv', delimiter=',', fmt='%s', newline='\\n', X=data_header[rankeds])\n",
    "        \n",
    "        filtered_dataset = dataset_std[:, rankeds[0:30]]\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(filtered_dataset), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(filtered_dataset[train,0:(filtered_dataset.shape[1] - 1)])\n",
    "            clf.fit(scaler.transform(filtered_dataset[train, 0:(filtered_dataset.shape[1] - 1)]), features_set[train,features_set.shape[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(filtered_dataset[test, 0:(filtered_dataset.shape[1] - 1)]))\n",
    "            scored = predictions == features_set[test, features_set.shape[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "        print(\"Taxa de acerto média atributos selecionados\", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        better_total_scores.append(sum(all_scores)/len(all_scores))\n",
    "    return total_scores, better_total_scores\n",
    "        #print(scores, sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nova implementação do classificador que agora não faz distinção entre dados pré e dados pós trigtimes (contínua)\n",
    "#    mat_full => dicionario contendo todos os spikes de todos os neuronios gravados de um animal\n",
    "#    mat => sinais LFP's\n",
    "#    trigtimes => tempos nos quais o animal tocou o nose poke da câmara central (s)\n",
    "#    labels => Do mesmo tamanho de 'trigtimes', indica para que lado o animal foi no trial (0 ou 1)\n",
    "#    fs => frequência de amostragem (Hz) (dos sinais LFP's)\n",
    "#    interesting_time_start => indica quanto tempo antes de cada trigtime a janela deslizante irá começar (s)\n",
    "#    interesting_time_end => indica quanto tempo depois de cada trigtime a janela deslizante irá parar (s)\n",
    "#    window_size => tamanho da janela deslizante (s)\n",
    "#    overlap => tamanho do avanço da janela deslizante (s)\n",
    "#    subject_name => nome do animal (só para gerar arquivos customizados)\n",
    "def classifyOverlappingContinuous(mat_full, mat, trigtimes, labels, fs, interesting_time_start, \n",
    "                                  interesting_time_end, window_size, overlap, subject_name):\n",
    "       \n",
    "    #np.random.shuffle(labels)  #para testar viés do classificador\n",
    "    \n",
    "    #selecionando os indices dos 30 melhores atributos\n",
    "    top_attrs = rankings(mat_full, mat, trigtimes, labels, 1000, 0, 1, 1, 1, subject_name)[0:30] #melhores 30 atributos entre 0s e 1s pós trigtimes\n",
    "    \n",
    "    # criando diretórios para guardar resultados intermediários e finais \n",
    "    if not os.path.exists('ranking'):\n",
    "        os.makedirs('ranking')\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "    if not os.path.exists('ranking/' + subject_name):\n",
    "        os.makedirs('ranking/' + subject_name)\n",
    "        os.makedirs('datasets/' + subject_name)\n",
    "    \n",
    "    #com deslize\n",
    "    if (overlap > 0):\n",
    "        numberOfLoops = int((interesting_time_end - interesting_time_start - window_size)/overlap)   #a divisão tem que dar inteira, atenção\n",
    "    else:\n",
    "        numberOfLoops = 0    #sem deslize (interessante para rodar a função uma única vez em um intervalo de tempo ex.: extrair os mellhores atributos)\n",
    "    print(numberOfLoops)\n",
    "    \n",
    "    # monta uma lista com os nomes de cada atributo\n",
    "    data_header = []\n",
    "    for l in spikes_labels:\n",
    "        data_header.append(l + '_num_spikes')\n",
    "        data_header.append(l + '_lat_spikes')\n",
    "    #data_header.append('delta_power1')\n",
    "    data_header.append('theta_power1')\n",
    "    data_header.append('alpha_power1')\n",
    "    data_header.append('beta_power1')\n",
    "    data_header.append('gamma_power1')\n",
    "    #data_header.append('delta_power2')\n",
    "    data_header.append('theta_power2')\n",
    "    data_header.append('alpha_power2')\n",
    "    data_header.append('beta_power2')\n",
    "    data_header.append('gamma_power2')\n",
    "    #data_header.append('delta_power3')\n",
    "    data_header.append('theta_power3')\n",
    "    data_header.append('alpha_power3')\n",
    "    data_header.append('beta_power3')\n",
    "    data_header.append('gamma_power3')\n",
    "    #data_header.append('delta_power4')\n",
    "    data_header.append('theta_power4')\n",
    "    data_header.append('alpha_power4')\n",
    "    data_header.append('beta_power4')\n",
    "    data_header.append('gamma_power4')\n",
    "    data_header.append('decision')\n",
    "    \n",
    "    #guardará a taxa de acerto para cada trial\n",
    "    total_scores = []\n",
    "    #guardará a taxa de acerto para cada trial com os atributos selecionados\n",
    "    better_total_scores = []\n",
    "    for loop in range(0, numberOfLoops + 1):   # +1 accounts for the first window, so if the number of loops == 0, it will run once\n",
    "        #trigtimes\n",
    "        eventIndexes = trigtimes[:,0]\n",
    "        #dataset for each trigtime (evento)\n",
    "        dataframe = []\n",
    "        \n",
    "        # pra cada trial ... extraia o dataset\n",
    "        for j in range(0, len(eventIndexes)):\n",
    "            print(\"\\r\",j,end='')\n",
    "            #guarda cada feature na ordem de data_header\n",
    "            event_features = []\n",
    "            #para cada neurônio, extrair os atributos: \"numero de spikes\", \"tempo médio entre spikes\"\n",
    "            for i in range(0, len(spikes_labels)):\n",
    "                spikeTimes = np.array(mat_full[spikes_labels[i]]) #in seconds\n",
    "                #fft lfp attributes\n",
    "                spikesCount = 0\n",
    "                SpikesList = []\n",
    "                \n",
    "                SpikesList = spikeTimes[(spikeTimes >= eventIndexes[j] + interesting_time_start + (loop * overlap)) &\n",
    "                                          (spikeTimes < eventIndexes[j] + interesting_time_start + window_size + (loop * overlap))]\n",
    "                \n",
    "                event_features.append(len(SpikesList))\n",
    "                event_features.append(spikesLatency(SpikesList))                \n",
    "            #extrai a potência de cada faixa de frequência de cada sinal LFP e concatena na lista de atributos  \n",
    "            event_features = event_features + lfpBandsExtractor(\n",
    "                mat[eventIndexes[j]*fs + interesting_time_start*fs + loop*overlap*fs:\n",
    "                    eventIndexes[j]*fs + interesting_time_start*fs +  window_size*fs + loop*overlap*fs], fs)\n",
    "            #concatena os labels no fim da lista de atributos\n",
    "            event_features.append(labels[j,0])\n",
    "            #adiciona a linha de atributos desse trial para o dataset\n",
    "            dataframe.append(event_features)\n",
    "            \n",
    "        print(np.shape(dataframe))\n",
    "        #salva o dataset na pasta datasets\n",
    "        np.savetxt('datasets/' + subject_name + '/' + 'dataset_' + str( interesting_time_start + (loop * overlap)) \n",
    "                   + '@' + str(interesting_time_start + window_size + (loop * overlap)) + '.csv', delimiter=',', fmt='%.8f',\n",
    "                    newline='\\n', header=\",\".join(data_header), X=dataframe)\n",
    "        #normalizing\n",
    "        \n",
    "        #transforma dataframe para numpy array\n",
    "        features_set = np.asarray(dataframe)        \n",
    "        \n",
    "        print(len(features_set))       \n",
    "        \n",
    "        #começa processo de classificação com todos atributos\n",
    "        print(\"    Rodando database normal...\")\n",
    "        #guarda todos os scores no processo de cross-validation\n",
    "        all_scores = []\n",
    "        #cross-validation usando k = raiz_quadrada da quantidade de trials\n",
    "        k_fold = cross_validation.KFold(len(features_set), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            #cria classificador SVM default\n",
    "            clf = svm.SVC()\n",
    "            #normalizando dados\n",
    "            scaler = preprocessing.StandardScaler().fit(features_set[train,0:np.shape(features_set)[1] - 1])\n",
    "            #treinando svm\n",
    "            clf.fit(scaler.transform(features_set[train,0:np.shape(features_set)[1] - 1]), features_set[train, np.shape(features_set)[1] - 1])\n",
    "            #classificando...\n",
    "            predictions = clf.predict(scaler.transform(features_set[test, 0:np.shape(features_set)[1] - 1]))\n",
    "            #acertos (array de 0's e 1's, 0-erro e 1-acerto)\n",
    "            scored = predictions == features_set[test, np.shape(features_set)[1] - 1]\n",
    "            #calcula acerto entre 0 e 1\n",
    "            all_scores.append(sum(scored)/len(scored)) \n",
    "        \n",
    "        #averages all_scores, resultando na taxa de acerto média para a janela de tempo em questão \n",
    "        print(\"Taxa de acerto média \", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        total_scores.append(sum(all_scores)/len(all_scores))\n",
    "        \n",
    "        #criando um classificador com os 30 melhores atributos\n",
    "        print(\"    Rodando database com os melhores atributos...\")\n",
    "        \n",
    "        #opcional, faz o ranqueiamento usando regressão logistica, de todos os atributos para esse trial e salva em um arquivo na pasta ranking\n",
    "        dataset_std = preprocessing.StandardScaler().fit_transform(features_set[:,0:np.shape(features_set)[1] - 1])\n",
    "        model = LogisticRegression()\n",
    "        rfe = RFE(model, 1)\n",
    "        fit = rfe.fit(dataset_std, features_set[:, np.shape(features_set)[1] - 1])\n",
    "        data_header = np.array(data_header)\n",
    "        rankeds = np.argsort(fit.ranking_)\n",
    "        \n",
    "        np.savetxt('ranking/' + subject_name + '/' + 'ranking_features_' + str(interesting_time_start + (loop * overlap)) \n",
    "                   + '@' + str(interesting_time_start + window_size + (loop * overlap)) + '.csv',\n",
    "                   delimiter=',', fmt='%s', newline='\\n', X=data_header[rankeds])\n",
    "        #fim opcional (rankings faz a mesma coisa, só não salva no arquivo)\n",
    "        \n",
    "        #novo dataset utilizando os 30 melhores atributos\n",
    "        filtered_dataset = dataset_std[:, top_attrs]\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(filtered_dataset), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(filtered_dataset[train,0:(filtered_dataset.shape[1] - 1)])\n",
    "            clf.fit(scaler.transform(filtered_dataset[train, 0:(filtered_dataset.shape[1] - 1)]), features_set[train,features_set.shape[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(filtered_dataset[test, 0:(filtered_dataset.shape[1] - 1)]))\n",
    "            scored = predictions == features_set[test, features_set.shape[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "        print(\"Taxa de acerto média atributos selecionados\", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        better_total_scores.append(sum(all_scores)/len(all_scores))\n",
    "    \n",
    "    # retorna a curva da taxa de acerto dos classificadores sem seleção de atributos e com, respectivamente\n",
    "    return total_scores, better_total_scores\n",
    "\n",
    "# segue a mesma documentação da função acima e faz a mesma coisa, só que no final, ranqueia os melhores atributos no processo de classificação\n",
    "def rankings(mat_full, mat, trigtimes, labels, fs, interesting_time_start, \n",
    "                                  interesting_time_end, window_size, overlap, subject_name):\n",
    "    \n",
    "    #np.random.shuffle(labels)\n",
    "    \n",
    "    if not os.path.exists('ranking'):\n",
    "        os.makedirs('ranking')\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "    if not os.path.exists('ranking/' + subject_name):\n",
    "        os.makedirs('ranking/' + subject_name)\n",
    "        os.makedirs('datasets/' + subject_name)\n",
    "        \n",
    "    #if (overlap > (interesting_time_end - interesting_time_start) - window_size):\n",
    "    #    raise Error('Erro')\n",
    "    if (overlap > 0):\n",
    "        numberOfLoops = int((interesting_time_end - interesting_time_start - window_size)/overlap)   #a divisão tem que dar inteira, atenção\n",
    "    else:\n",
    "        numberOfLoops = 0\n",
    "    print('Número de Loops no rankings', numberOfLoops)\n",
    "    \n",
    "    data_header = []\n",
    "    for l in spikes_labels:\n",
    "        data_header.append(l + '_num_spikes')\n",
    "        data_header.append(l + '_lat_spikes')\n",
    "    #data_header.append('delta_power1')\n",
    "    data_header.append('theta_power1')\n",
    "    data_header.append('alpha_power1')\n",
    "    data_header.append('beta_power1')\n",
    "    data_header.append('gamma_power1')\n",
    "    #data_header.append('delta_power2')\n",
    "    data_header.append('theta_power2')\n",
    "    data_header.append('alpha_power2')\n",
    "    data_header.append('beta_power2')\n",
    "    data_header.append('gamma_power2')\n",
    "    #data_header.append('delta_power3')\n",
    "    data_header.append('theta_power3')\n",
    "    data_header.append('alpha_power3')\n",
    "    data_header.append('beta_power3')\n",
    "    data_header.append('gamma_power3')\n",
    "    #data_header.append('delta_power4')\n",
    "    data_header.append('theta_power4')\n",
    "    data_header.append('alpha_power4')\n",
    "    data_header.append('beta_power4')\n",
    "    data_header.append('gamma_power4')\n",
    "    data_header.append('decision')\n",
    "    \n",
    "    total_scores = []\n",
    "    better_total_scores = []\n",
    "    for loop in range(0, numberOfLoops + 1):   # +1 accounts for the first window\n",
    "        eventIndexes = trigtimes[:,0]\n",
    "        results = ['D', 'E']\n",
    "        dataframe = []\n",
    "\n",
    "        searchIndexes = [0] * len(spikes_labels)\n",
    "\n",
    "        for j in range(0, len(eventIndexes)):\n",
    "            print(\"\\r\",j,end='')\n",
    "            event_features = []\n",
    "            for i in range(0, len(spikes_labels)):\n",
    "                spikeTimes = np.array(mat_full[spikes_labels[i]]) #in seconds\n",
    "                #fft lfp attributes\n",
    "                spikesCount = 0\n",
    "                posSpikesCount = 0\n",
    "                preSpikesList = []\n",
    "                \n",
    "                #pre\n",
    "                preSpikesList = spikeTimes[(spikeTimes >= eventIndexes[j] + interesting_time_start + (loop * overlap)) &\n",
    "                                          (spikeTimes < eventIndexes[j] + interesting_time_start + window_size + (loop * overlap))]\n",
    "                \n",
    "                event_features.append(len(preSpikesList))\n",
    "                event_features.append(spikesLatency(preSpikesList))                \n",
    "            #pre    \n",
    "            event_features = event_features + lfpBandsExtractor(\n",
    "                mat[eventIndexes[j]*fs + interesting_time_start*fs + loop*overlap*fs:\n",
    "                    eventIndexes[j]*fs + interesting_time_start*fs +  window_size*fs + loop*overlap*fs], fs)\n",
    "            \n",
    "            event_features.append(labels[j,0])            \n",
    "            dataframe.append(event_features)        \n",
    "        #normalizing\n",
    "        \n",
    "        features_set = np.asarray(dataframe)        \n",
    "        \n",
    "        print(len(features_set))       \n",
    "        \n",
    "        print(\"    Rodando database normal...\")\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(features_set), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(features_set[train,0:np.shape(features_set)[1] - 1])\n",
    "            clf.fit(scaler.transform(features_set[train,0:np.shape(features_set)[1] - 1]), features_set[train, np.shape(features_set)[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(features_set[test, 0:np.shape(features_set)[1] - 1]))\n",
    "            scored = predictions == features_set[test, np.shape(features_set)[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "            #print(predictions)\n",
    "        #scores = cross_validation.cross_val_score(\n",
    "        #clf, X_scaled, features_set[:,164], cv=5)\n",
    "        print(\"Taxa de acerto média \", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        total_scores.append(sum(all_scores)/len(all_scores))\n",
    "        \n",
    "        dataset_std = preprocessing.StandardScaler().fit_transform(features_set[:,0:np.shape(features_set)[1] - 1])\n",
    "        model = LogisticRegression()\n",
    "        rfe = RFE(model, 1)\n",
    "        fit = rfe.fit(dataset_std, features_set[:, np.shape(features_set)[1] - 1])\n",
    "        data_header = np.array(data_header)\n",
    "        rankeds = np.argsort(fit.ranking_)\n",
    "        return rankeds\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#só backup antigo\n",
    "def classifyOverlappingContinuous_backup(mat_full, mat, trigtimes, labels, fs, interesting_time_start, \n",
    "                                  interesting_time_end, window_size, overlap, subject_name):\n",
    "    \n",
    "    #np.random.shuffle(labels)\n",
    "    \n",
    "    if not os.path.exists('ranking'):\n",
    "        os.makedirs('ranking')\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.makedirs('datasets')\n",
    "    if not os.path.exists('ranking/' + subject_name):\n",
    "        os.makedirs('ranking/' + subject_name)\n",
    "        os.makedirs('datasets/' + subject_name)\n",
    "        \n",
    "    #if (overlap > (interesting_time_end - interesting_time_start) - window_size):\n",
    "    #    raise Error('Erro')\n",
    "    if (overlap > 0):\n",
    "        numberOfLoops = int((interesting_time_end - interesting_time_start - window_size)/overlap)   #a divisão tem que dar inteira, atenção\n",
    "    else:\n",
    "        numberOfLoops = 0\n",
    "    print(numberOfLoops)\n",
    "    \n",
    "    data_header = []\n",
    "    for l in spikes_labels:\n",
    "        data_header.append(l + '_num_spikes')\n",
    "        data_header.append(l + '_lat_spikes')\n",
    "    #data_header.append('delta_power1')\n",
    "    data_header.append('theta_power1')\n",
    "    data_header.append('alpha_power1')\n",
    "    data_header.append('beta_power1')\n",
    "    data_header.append('gamma_power1')\n",
    "    #data_header.append('delta_power2')\n",
    "    data_header.append('theta_power2')\n",
    "    data_header.append('alpha_power2')\n",
    "    data_header.append('beta_power2')\n",
    "    data_header.append('gamma_power2')\n",
    "    #data_header.append('delta_power3')\n",
    "    data_header.append('theta_power3')\n",
    "    data_header.append('alpha_power3')\n",
    "    data_header.append('beta_power3')\n",
    "    data_header.append('gamma_power3')\n",
    "    #data_header.append('delta_power4')\n",
    "    data_header.append('theta_power4')\n",
    "    data_header.append('alpha_power4')\n",
    "    data_header.append('beta_power4')\n",
    "    data_header.append('gamma_power4')\n",
    "    data_header.append('decision')\n",
    "    \n",
    "    total_scores = []\n",
    "    better_total_scores = []\n",
    "    for loop in range(0, numberOfLoops + 1):   # +1 accounts for the first window\n",
    "        eventIndexes = trigtimes[:,0]\n",
    "        results = ['D', 'E']\n",
    "        dataframe = []\n",
    "\n",
    "        searchIndexes = [0] * len(spikes_labels)\n",
    "\n",
    "        for j in range(0, len(eventIndexes)):\n",
    "            print(\"\\r\",j,end='')\n",
    "            event_features = []\n",
    "            for i in range(0, len(spikes_labels)):\n",
    "                spikeTimes = np.array(mat_full[spikes_labels[i]]) #in seconds\n",
    "                #fft lfp attributes\n",
    "                spikesCount = 0\n",
    "                posSpikesCount = 0\n",
    "                preSpikesList = []\n",
    "                \n",
    "                #pre\n",
    "                preSpikesList = spikeTimes[(spikeTimes >= eventIndexes[j] + interesting_time_start + (loop * overlap)) &\n",
    "                                          (spikeTimes < eventIndexes[j] + interesting_time_start + window_size + (loop * overlap))]\n",
    "                \n",
    "                event_features.append(len(preSpikesList))\n",
    "                event_features.append(spikesLatency(preSpikesList))                \n",
    "            #pre    \n",
    "            event_features = event_features + lfpBandsExtractor(\n",
    "                mat[eventIndexes[j]*fs + interesting_time_start*fs + loop*overlap*fs:\n",
    "                    eventIndexes[j]*fs + interesting_time_start*fs +  window_size*fs + loop*overlap*fs], fs)\n",
    "            \n",
    "            event_features.append(labels[j,0])            \n",
    "            dataframe.append(event_features)\n",
    "            \n",
    "        print(np.shape(dataframe))\n",
    "        np.savetxt('datasets/' + subject_name + '/' + 'dataset_' + str( interesting_time_start + (loop * overlap)) \n",
    "                   + '@' + str(interesting_time_start + window_size + (loop * overlap)) + '.csv', delimiter=',', fmt='%.8f',\n",
    "                    newline='\\n', X=dataframe)\n",
    "        #normalizing\n",
    "        \n",
    "        features_set = np.asarray(dataframe)        \n",
    "        \n",
    "        print(len(features_set))       \n",
    "        \n",
    "        print(\"    Rodando database normal...\")\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(features_set), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(features_set[train,0:np.shape(features_set)[1] - 1])\n",
    "            clf.fit(scaler.transform(features_set[train,0:np.shape(features_set)[1] - 1]), features_set[train, np.shape(features_set)[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(features_set[test, 0:np.shape(features_set)[1] - 1]))\n",
    "            scored = predictions == features_set[test, np.shape(features_set)[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "            #print(predictions)\n",
    "        #scores = cross_validation.cross_val_score(\n",
    "        #clf, X_scaled, features_set[:,164], cv=5)\n",
    "        print(\"Taxa de acerto média \", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        total_scores.append(sum(all_scores)/len(all_scores))\n",
    "        \n",
    "        print(\"    Rodando database com os melhores atributos...\")\n",
    "        \n",
    "        dataset_std = preprocessing.StandardScaler().fit_transform(features_set[:,0:np.shape(features_set)[1] - 1])\n",
    "        model = LogisticRegression()\n",
    "        rfe = RFE(model, 1)\n",
    "        fit = rfe.fit(dataset_std, features_set[:, np.shape(features_set)[1] - 1])\n",
    "        data_header = np.array(data_header)\n",
    "        rankeds = np.argsort(fit.ranking_)\n",
    "        \n",
    "        np.savetxt('ranking/' + subject_name + '/' + 'ranking_features_' + str(interesting_time_start + (loop * overlap)) \n",
    "                   + '@' + str(interesting_time_start + window_size + (loop * overlap)) + '.csv',\n",
    "                   delimiter=',', fmt='%s', newline='\\n', X=data_header[rankeds])\n",
    "        \n",
    "        filtered_dataset = dataset_std[:, rankeds[0:30]]\n",
    "        all_scores = []\n",
    "        k_fold = cross_validation.KFold(len(filtered_dataset), int(np.sqrt(np.shape(features_set)[0])), random_state=151515)\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            clf = svm.SVC()\n",
    "            #print(\"treino:\", len(train), \"teste:\", len(test))\n",
    "            scaler = preprocessing.StandardScaler().fit(filtered_dataset[train,0:(filtered_dataset.shape[1] - 1)])\n",
    "            clf.fit(scaler.transform(filtered_dataset[train, 0:(filtered_dataset.shape[1] - 1)]), features_set[train,features_set.shape[1] - 1])\n",
    "            predictions = clf.predict(scaler.transform(filtered_dataset[test, 0:(filtered_dataset.shape[1] - 1)]))\n",
    "            scored = predictions == features_set[test, features_set.shape[1] - 1]\n",
    "            #print(\"Taxa de acerto cross: \" + str(sum(scored)/len(scored)))\n",
    "            all_scores.append(sum(scored)/len(scored))\n",
    "        print(\"Taxa de acerto média atributos selecionados\", int(np.sqrt(np.shape(features_set)[0])), \"fold:\", sum(all_scores)/len(all_scores))\n",
    "        better_total_scores.append(sum(all_scores)/len(all_scores))\n",
    "    return total_scores, better_total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#variáveis importantes para o funcionamento de todas as funções\n",
    "\n",
    "#lista dos nomes de todos os arquivos \".mat\" de spikes \n",
    "spikes_files = sorted(os.listdir(\"spikes_right\"))\n",
    "#lista dos nomes de todos os arquivos \".mat\" de lfp's \n",
    "lfps_files = sorted(os.listdir(\"lfps\"))\n",
    "#lista dos nomes de todos os arquivos \".mat\" de labels \n",
    "labels_files = sorted(os.listdir(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#para cada animal...\n",
    "for f_index in range(0, len(spikes_files)):\n",
    "    #carrega o dicionário de neurônios\n",
    "    mat_full = sci2.loadmat('spikes_right/' + spikes_files[f_index])\n",
    "    #carrega só os sinais LFP's do .mat de LFP\n",
    "    mat = sci2.loadmat('lfps/' + lfps_files[f_index])['LFP']\n",
    "    #carrega os labels\n",
    "    labels = sci2.loadmat('labels/' + labels_files[f_index])['labels']\n",
    "    #carrega os trigtimes\n",
    "    trigtimes = sci2.loadmat('labels/' + labels_files[f_index])['trigtimes']\n",
    "    #filtra só as chaves importantes no dicionário do .mat de spikes\n",
    "    spikes_labels = list(key for key in mat_full.keys() if key.startswith('SI')\n",
    "                         or key.startswith('PFC') or key.startswith('VI') or key.startswith('PPC'))\n",
    "    \n",
    "    #pega as curvas da taxa de acerto para o animal spikes_files[f_index], de -4 a 4 segundos com uma janela de 0.5s deslizando em 0.025s\n",
    "    learning_curve, better_learning_curve = classifyOverlappingContinuous(mat_full, mat, trigtimes, labels, 1000, -4, 4, 0.5, 0.025, spikes_files[f_index])\n",
    "    \n",
    "    #dataset, labels = classifyOverlappingContinuous(mat_full, mat, trigtimes, labels, 1000, -0.5, 4, 0.5, 0.025, spikes_files[f_index])\n",
    "    #learning_curve, better_learning_curve = classifyOverlappingContinuous(mat_full, mat, trigtimes, labels, 1000, 0, 1, 1, 1, spikes_files[f_index])\n",
    "    \n",
    "    #guarda curvas em arquivos\n",
    "    if not os.path.exists('learning-curves'):\n",
    "        os.makedirs('learning-curves')\n",
    "    if not os.path.exists('learning-curves/' + spikes_files[f_index].split('.')[0]):\n",
    "        os.makedirs('learning-curves/' + spikes_files[f_index].split('.')[0])\n",
    "    np.savetxt('learning-curves/' + spikes_files[f_index].split('.')[0] + '/' + 'posrates_4segs_0.5window_size_0.025overlapping.csv' , delimiter=',', fmt='%.14f', newline='\\n', X=learning_curve)\n",
    "    np.savetxt('learning-curves/' + spikes_files[f_index].split('.')[0] + '/' + 'better_posrates_4segs_0.5window_size_0.025overlapping.csv' , delimiter=',', fmt='%.14f', newline='\\n', X=better_learning_curve)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def gen_hex_colour_code():\n",
    "   return '#'+ ''.join([random.choice('0123456789abcdef') for x in range(6)])\n",
    "gen_hex_colour_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#proccess results\n",
    "\n",
    "results_folder = \"results_pos\"\n",
    "results_folder_pre = \"results_pre\"\n",
    "\n",
    "\n",
    "results_files = os.listdir(results_folder)\n",
    "results_files_pre = os.listdir(results_folder_pre)\n",
    "\n",
    "rats = set()\n",
    "for f in results_files:\n",
    "    fn = f.split('WD')[0]\n",
    "    fn = fn[0: len(fn) - 1]\n",
    "    rats.add(fn)\n",
    "\n",
    "better_curves = []\n",
    "worse_curves = []\n",
    "    \n",
    "for rat in rats:\n",
    "    print(rat)\n",
    "    rat_files = list(k for k in results_files if k.startswith(rat))\n",
    "    graphical_files = list(k for k in rat_files if 'overlapping' in k)\n",
    "    sequencial_files = list(k for k in rat_files if 'overlapping' not in k)\n",
    "    better_file = list(k for k in graphical_files if 'better' in k)\n",
    "    worse_file = list(k for k in graphical_files if 'better' not in k)\n",
    "    features_evolution = len(sequencial_files)*[None]\n",
    "    \n",
    "    rat_files_pre = list(k for k in results_files_pre if k.startswith(rat))\n",
    "    graphical_files_pre = list(k for k in rat_files_pre if 'overlapping' in k)\n",
    "    sequencial_files_pre = list(k for k in rat_files_pre if 'overlapping' not in k)\n",
    "    better_file_pre = list(k for k in graphical_files_pre if 'better' in k)\n",
    "    worse_file_pre = list(k for k in graphical_files_pre if 'better' not in k)\n",
    "    features_evolution_pre = len(sequencial_files_pre)*[None]\n",
    "    \n",
    "    for sf in sequencial_files:\n",
    "        ind = sf.split('_')\n",
    "        ind = ind[len(ind) - 1]\n",
    "        ind = int(ind.split('.')[0])\n",
    "        features_evolution[ind] = np.loadtxt(results_folder+'/'+sf, dtype=bytes, delimiter=\"\\n\").astype(str)\n",
    "    for sf in sequencial_files_pre:\n",
    "        ind = sf.split('_')\n",
    "        ind = ind[len(ind) - 1]\n",
    "        ind = int(ind.split('.')[0])\n",
    "        features_evolution_pre[ind] = np.loadtxt(results_folder_pre+'/'+sf, dtype=bytes, delimiter=\"\\n\").astype(str)\n",
    "        \n",
    "    better = np.loadtxt(results_folder+'/'+better_file[0])\n",
    "    worse = np.loadtxt(results_folder+'/'+worse_file[0])\n",
    "    \n",
    "    better_pre = np.loadtxt(results_folder_pre+'/'+better_file_pre[0])\n",
    "    worse_pre = np.loadtxt(results_folder_pre+'/'+worse_file_pre[0])\n",
    "    \n",
    "    better_curves.append(np.hstack((better_pre, better)))\n",
    "    worse_curves.append(np.hstack((worse_pre, worse)))\n",
    "    \n",
    "    mtp.rcParams['xtick.labelsize'] = 14 \n",
    "    mtp.rcParams['ytick.labelsize'] = 14 \n",
    "    mtp.figure(figsize=(20,14))\n",
    "    xaxis = np.linspace(0, 4, 141)\n",
    "    mtp.title(rat + ' learning rates', fontsize=20)\n",
    "    mtp.xlabel('Tempo (s)', fontsize=20)\n",
    "    mtp.ylabel('Taxa de acerto [0,1]', fontsize=20)\n",
    "    axes = mtp.gca()\n",
    "    axes.set_ylim([0.4,1])\n",
    "    axes.set_yticks(np.arange(0.4, 1.05, 0.05))\n",
    "    axes.set_xticks(np.arange(-4, 4.25, 0.5))\n",
    "    #mtp.yticks = np.arange(0.4, 1.05, 0.05)\n",
    "    \n",
    "    mtp.axvline(x=0, linewidth=0.5, linestyle='dashed', color='gray')\n",
    "    \n",
    "    for coord in np.arange(0.4, 1.05, 0.05):\n",
    "        mtp.axhline(y=coord, linewidth=0.5, linestyle='dashed', color='blue')\n",
    "    \n",
    "    mtp.plot(xaxis, worse, 'r-',xaxis, better, 'g-', xaxis[::-1]*-1, worse_pre, 'r-', xaxis[::-1]*-1, better_pre, 'g-')\n",
    "    mtp.savefig(rat + '_full_learning_rates.png', dpi=200)\n",
    "    print('plotou')\n",
    "    features_rankings = {}\n",
    "    features_rankings_pre = {}\n",
    "    for i in features_evolution[0]:\n",
    "        features_rankings[i] = []\n",
    "        features_rankings_pre[i] = []\n",
    "    mtp.close()\n",
    "    #rank of each feature per time\n",
    "    for fe in range(0, len(features_evolution)):\n",
    "        for rank in range(0, len(features_evolution[fe])):\n",
    "            features_rankings[features_evolution[fe][rank]].append(rank)\n",
    "    #mtp.figure(figsize=(30,20))\n",
    "    #for feature in (list(features_rankings.keys())[0:4]):\n",
    "    #    mtp.plot(features_rankings[feature])\n",
    "    #mtp.savefig('kappapride', dpi=200)\n",
    "    \n",
    "    #mtp.clf()  \n",
    "    \n",
    "    for fe in range(0, len(features_evolution_pre)):\n",
    "        for rank in range(0, len(features_evolution_pre[fe])):\n",
    "            features_rankings_pre[features_evolution_pre[fe][rank]].append(rank)\n",
    "    #mtp.figure(figsize=(30,20))\n",
    "    #for feature in (list(features_rankings_pre.keys())[0:4]):\n",
    "    #    mtp.plot(features_rankings_pre[feature])\n",
    "    #mtp.savefig('kappapride2', dpi=200)\n",
    "    \n",
    "    #mtp.clf()\n",
    "    \n",
    "    #appearance of each feature at top30 over time\n",
    "    all_labels = list(features_rankings.keys())\n",
    "    print(\"Rato: \", rat, \"Eletrodos: \", (len(all_labels) - 20)/2)\n",
    "    continue\n",
    "    #print(all_labels)\n",
    "    mtp.figure(figsize=(20,14))\n",
    "    top = 30\n",
    "    axes = mtp.gca()\n",
    "    axes.set_xticks(np.arange(-4, 4.5, 0.5))\n",
    "    #for i in range(0, len(features_rankings[all_labels[0]])):\n",
    "    #    y_values = []\n",
    "    #    for j in range(0, len(all_labels)):\n",
    "    #        if features_rankings[all_labels[j]][i] < top:\n",
    "    #            y_values.append(j)\n",
    "    #        else:\n",
    "    #            y_values.append(0)\n",
    "    #    mtp.plot([i] * len(y_values), y_values, 'b*')    \n",
    "    \n",
    "    regions_evolution = {}\n",
    "    \n",
    "    regions_evolution['PFC'] = [0] * len(features_rankings[all_labels[0]])\n",
    "    regions_evolution['PPC'] = [0] * len(features_rankings[all_labels[0]])\n",
    "    regions_evolution['SI'] = [0] * len(features_rankings[all_labels[0]])\n",
    "    regions_evolution['VI'] = [0] * len(features_rankings[all_labels[0]])\n",
    "    #regions_evolution['LFP_related'] = [0] * len(features_rankings[all_labels[0]])\n",
    "    #measuring region importance\n",
    "    for i in range(0, len(features_rankings[all_labels[0]])):\n",
    "        for lab in all_labels:\n",
    "            if(features_rankings[lab][i] < top):\n",
    "                if(lab.startswith('PFC')):\n",
    "                    regions_evolution['PFC'][i] += 1\n",
    "                elif(lab.startswith('PPC')):\n",
    "                    regions_evolution['PPC'][i] += 1\n",
    "                elif(lab.startswith('SI')):\n",
    "                    regions_evolution['SI'][i] += 1\n",
    "                elif(lab.startswith('VI')):\n",
    "                    regions_evolution['VI'][i] += 1\n",
    "                elif(lab.endswith('1')):\n",
    "                    regions_evolution_pre['PFC'][i] += 1\n",
    "                elif(lab.endswith('2')):\n",
    "                    regions_evolution_pre['PPC'][i] += 1\n",
    "                elif(lab.endswith('3')):\n",
    "                    regions_evolution_pre['SI'][i] += 1\n",
    "                elif(lab.endswith('4')):\n",
    "                    regions_evolution_pre['VI'][i] += 1\n",
    "    \n",
    "    #for region in regions_evolution.keys():\n",
    "    #    mtp.plot(regions_evolution[region], label=region )\n",
    "    #mtp.legend()\n",
    "    #mtp.savefig(rat + '_regions_importance_evolution_pos.png', dpi=200)\n",
    "    \n",
    "    #mtp.clf()\n",
    "    regions_evolution_pre = {}\n",
    "    \n",
    "    regions_evolution_pre['PFC'] = [0] * len(features_rankings_pre[all_labels[0]])\n",
    "    regions_evolution_pre['PPC'] = [0] * len(features_rankings_pre[all_labels[0]])\n",
    "    regions_evolution_pre['SI'] = [0] * len(features_rankings_pre[all_labels[0]])\n",
    "    regions_evolution_pre['VI'] = [0] * len(features_rankings_pre[all_labels[0]])\n",
    "    #regions_evolution_pre['LFP_related'] = [0] * len(features_rankings_pre[all_labels[0]])\n",
    "    #measuring region importance\n",
    "    for i in range(0, len(features_rankings_pre[all_labels[0]])):\n",
    "        for lab in all_labels:\n",
    "            if(features_rankings_pre[lab][i] < top):\n",
    "                if(lab.startswith('PFC')):\n",
    "                    regions_evolution_pre['PFC'][i] += 1\n",
    "                elif(lab.startswith('PPC')):\n",
    "                    regions_evolution_pre['PPC'][i] += 1\n",
    "                elif(lab.startswith('SI')):\n",
    "                    regions_evolution_pre['SI'][i] += 1\n",
    "                elif(lab.startswith('VI')):\n",
    "                    regions_evolution_pre['VI'][i] += 1\n",
    "                elif(lab.endswith('1')):\n",
    "                    regions_evolution_pre['PFC'][i] += 1\n",
    "                elif(lab.endswith('2')):\n",
    "                    regions_evolution_pre['PPC'][i] += 1\n",
    "                elif(lab.endswith('3')):\n",
    "                    regions_evolution_pre['SI'][i] += 1\n",
    "                elif(lab.endswith('4')):\n",
    "                    regions_evolution_pre['VI'][i] += 1\n",
    "    \n",
    "    for region in regions_evolution_pre.keys():\n",
    "        mtp.plot(np.linspace(-4, 4, num=len(regions_evolution_pre[region] + regions_evolution[region])), \n",
    "                 regions_evolution_pre[region] + regions_evolution[region], label=region, linewidth=3 )\n",
    "    mtp.legend()\n",
    "    mtp.xlabel('Tempo(s)')\n",
    "    mtp.ylabel('Número de atributos no top ' + str(top), fontsize=16)\n",
    "    mtp.axvline(x=0, color = 'black', linewidth = 1, linestyle='dashed')\n",
    "    mtp.title(rat + ' - Contribuição de cada região na predição (top '+ str(top) + ')', fontsize=16)\n",
    "    mtp.savefig(rat + '_regions_importance_evolution.png', dpi=200)\n",
    "    \n",
    "    mtp.close()  \n",
    "    to_label = []\n",
    "    to_label_names = []\n",
    "    mtp.figure(figsize=(20,14))\n",
    "    mtp.xlim((-4, 4))\n",
    "    mtp.ylim((0, len(all_labels) + 1))\n",
    "    for i in range(0, len(all_labels)):\n",
    "        x_values = []\n",
    "        all_ranks = np.array(features_rankings[all_labels[i]])\n",
    "        all_ranks_pre = np.array(features_rankings_pre[all_labels[i]])\n",
    "        #print(\"all ranks\", all_ranks)\n",
    "        mask_below_top = (all_ranks < top)\n",
    "        mask_above_top = (all_ranks >= top)\n",
    "        mask_below_top_pre = (all_ranks_pre < top)\n",
    "        mask_above_top_pre = (all_ranks_pre >= top)\n",
    "        all_ranks[mask_above_top] = -1\n",
    "        all_ranks[mask_below_top] = i  \n",
    "        all_ranks_pre[mask_above_top_pre] = -1\n",
    "        all_ranks_pre[mask_below_top_pre] = i  \n",
    "        #if(sum(mask_below_top.astype(int)) > 0 or sum(mask_below_top_pre.astype(int)) > 0):\n",
    "        #    to_label.append(i)\n",
    "        #    to_label_names.append(all_labels[i])\n",
    "        to_label.append(i)\n",
    "        to_label_names.append(all_labels[i])\n",
    "        mtp.plot(np.linspace(-4, 4, num=len(np.hstack((all_ranks_pre, all_ranks)))), np.hstack((all_ranks_pre, all_ranks)), 'go', markersize=6)\n",
    "    mtp.axvline(x=0, color = 'b', linewidth = 1.5, linestyle = 'dashed')\n",
    "    mtp.yticks(to_label, to_label_names, rotation='horizontal', fontsize=6)\n",
    "    mtp.xlabel('Tempo (s)')\n",
    "    mtp.title(rat + ' - Melhores ' + str(top) + ' atributos por janela de tempo', fontsize=16)\n",
    "    #mtp.legend()    \n",
    "    mtp.savefig(rat + '_top' + str(top) + '_features_ranking.png', dpi=200)\n",
    "    \n",
    "    \n",
    "    mtp.clf()\n",
    "    for key in features_rankings.keys():\n",
    "        ranks_variation = np.array(features_rankings[key])\n",
    "        mtp.plot(np.arange(0, len(ranks_variation), 1), (ranks_variation < 30).astype(int), 'y*', label=key)\n",
    "    mtp.close()\n",
    "\n",
    "sum_better = np.zeros(len(better_curves[0]))\n",
    "sum_worse = np.zeros(len(worse_curves[0]))\n",
    "for i in range(0, len(better_curves)):\n",
    "    sum_better = sum_better + better_curves[i]\n",
    "    sum_worse = sum_worse + worse_curves[i]\n",
    "sum_better = sum_better/len(rats)   \n",
    "sum_worse = sum_worse/len(rats)   \n",
    "better_std = np.std(np.asarray(better_curves), axis=0)\n",
    "print(np.shape(np.array(better_std)))\n",
    "print(\"std\", better_std)\n",
    "print(np.shape(sum_better))\n",
    "worse_std = np.std(np.asarray(worse_curves), axis=0)\n",
    "mtp.figure(figsize=(20,14))\n",
    "mtp.plot(np.linspace(-4, 4, num=len(sum_better)), sum_better, label='Curva com seleção de atributos')\n",
    "mtp.plot(np.linspace(-4, 4, num=len(sum_better)), sum_worse, label='Curva sem seleção de atributos')\n",
    "mtp.legend()\n",
    "mtp.xlabel('Tempo(s)')\n",
    "mtp.ylabel('Taxa de acerto [0,1]', fontsize=16)\n",
    "mtp.axvline(x=0, color = 'black', linewidth = 1, linestyle='dashed')\n",
    "mtp.title('Taxa de acerto média para a população de ratos', fontsize=16)\n",
    "mtp.savefig('desempenho_médio_ratos.png', dpi=200)\n",
    "mtp.close()\n",
    "mtp.figure(figsize=(20,14))\n",
    "mtp.ylim((0.4, 1))\n",
    "mtp.errorbar(np.linspace(-4, 4, num=len(sum_better)), sum_better, yerr=better_std , fmt='--o')\n",
    "mtp.xlabel('Tempo(s)')\n",
    "mtp.ylabel('Taxa de acerto [0,1]', fontsize=16)\n",
    "mtp.axvline(x=0, color = 'black', linewidth = 1, linestyle='dashed')\n",
    "mtp.title('Taxa de acerto média para a população de ratos com seleção de atributos', fontsize=16)\n",
    "mtp.savefig('desempenho_médio_ratos_better_deviations.png', dpi=200)\n",
    "mtp.close()\n",
    "mtp.figure(figsize=(20,14))\n",
    "mtp.ylim((0.4, 1))\n",
    "mtp.errorbar(np.linspace(-4, 4, num=len(sum_better)), sum_worse, yerr=worse_std , fmt='--o')\n",
    "mtp.xlabel('Tempo(s)')\n",
    "mtp.ylabel('Taxa de acerto [0,1]', fontsize=16)\n",
    "mtp.axvline(x=0, color = 'black', linewidth = 1, linestyle='dashed')\n",
    "mtp.title('Taxa de acerto média para a população de ratos sem seleção de atributos', fontsize=16)\n",
    "mtp.savefig('desempenho_médio_ratos_worse_deviations.png', dpi=200)\n",
    "mtp.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
